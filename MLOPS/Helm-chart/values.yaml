# --- Model Image Configuration ---
image:
  repository: mycompanyacr.azurecr.io/model-inference-api
  tag: "1.0.0"  # This will be replaced by the specific MLflow/CI-built version
  pullPolicy: IfNotPresent

# --- Deployment Configuration ---
replicaCount: 2  # Recommended for high availability and zero-downtime
labels:
  app: ml-inference

# --- Resource Configuration (Crucial for MLOps) ---
resources:
  limits:
    cpu: 1000m  # 1 CPU core limit
    memory: 2Gi # 2 GiB memory limit
  requests:
    cpu: 500m
    memory: 1Gi

# --- Service Exposure (Internal) ---
service:
  type: ClusterIP  # Internal service for consumption by other microservices
  port: 80

# --- Ingress (External Access) ---
ingress:
  enabled: true
  host: api.ideogen.ai
  path: /v1/predict

# --- Health Check (Crucial for Deployment Strategy) ---
readinessProbe:
  enabled: true
  path: /healthz
  initialDelaySeconds: 15
  periodSeconds: 10
